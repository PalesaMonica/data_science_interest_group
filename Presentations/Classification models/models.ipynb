{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Introduction to Classification Models\n",
    "\n",
    "**What is Classification?**\n",
    "- Predicting categories (like spam/not spam) instead of numbers\n",
    "- A fundamental machine learning task\n",
    "\n",
    "**Real-World Examples:**\n",
    "- ğŸ“§ Email spam detection\n",
    "- ğŸ¥ Medical diagnosis (healthy/sick)\n",
    "- ğŸ“¸ Image recognition (cat/dog)\n",
    "- ğŸ’³ Fraud detection (fraudulent/legitimate)\n",
    "\n",
    "**Today's Goal:** Learn 5 simple but powerful classification models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep",
   "metadata": {},
   "source": [
    "## Preparing Our Data\n",
    "\n",
    "### Why Split Data?\n",
    "- ï¿½ Training set (70-80%): Teach the model\n",
    "- ğŸ§ª Test set (20-30%): Evaluate real performance\n",
    "- Prevents \"cheating\" by memorizing answers\n",
    "\n",
    "### Data Scaling\n",
    "- Some models need features on the same scale\n",
    "- Like comparing apples to apples ğŸğŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "data-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our practice data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create synthetic dataset (1000 examples, 10 features)\n",
    "X, y = make_classification(n_samples=1000, n_features=10, \n",
    "                           n_classes=2, random_state=42)\n",
    "\n",
    "# Split data (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features for models that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-intro",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression\n",
    "\n",
    "ğŸ“Š **How it works:** \n",
    "- Draws a \"best fit\" line between categories\n",
    "- Uses probability (sigmoid function) to classify\n",
    "\n",
    "ğŸ‘ **Best for:**\n",
    "- When the relationship is somewhat linear\n",
    "- Quick baseline model\n",
    "\n",
    "ğŸ‘ **Limitations:**\n",
    "- Struggles with complex patterns\n",
    "\n",
    "ğŸ” **Example Use:** Email spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "logistic-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "accuracy = log_reg.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knn-intro",
   "metadata": {},
   "source": [
    "## Model 2: k-Nearest Neighbors (k-NN)\n",
    "\n",
    "ğŸ“Š **How it works:** \n",
    "- \"Birds of a feather flock together\" ğŸ¦ğŸ¦\n",
    "- Classifies based on what the k closest points are\n",
    "\n",
    "ğŸ‘ **Best for:**\n",
    "- Simple, intuitive approach\n",
    "- Works well with clear groupings\n",
    "\n",
    "ğŸ‘ **Limitations:**\n",
    "- Slow with large datasets\n",
    "- Needs careful k selection\n",
    "\n",
    "ğŸ” **Example Use:** Handwriting recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "knn-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # Try k=5 neighbors\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "accuracy = knn.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tree-intro",
   "metadata": {},
   "source": [
    "## Model 3: Decision Trees\n",
    "\n",
    "ğŸ“Š **How it works:** \n",
    "- Series of yes/no questions like a flowchart â“â†’âœ“\n",
    "- Builds a tree by finding best splits\n",
    "\n",
    "ğŸ‘ **Best for:**\n",
    "- Easy to understand and visualize\n",
    "- Handles non-linear relationships\n",
    "\n",
    "ğŸ‘ **Limitations:**\n",
    "- Can overfit easily\n",
    "- Unstable with small data changes\n",
    "\n",
    "ğŸ” **Example Use:** Customer segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tree-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3)  # Limit tree depth\n",
    "tree.fit(X_train, y_train)\n",
    "accuracy = tree.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forest-intro",
   "metadata": {},
   "source": [
    "## Model 4: Random Forests\n",
    "\n",
    "ğŸ“Š **How it works:** \n",
    "- Many decision trees voting together ğŸŒ³ğŸŒ³ğŸŒ³â†’âœ‰\n",
    "- More accurate than single trees\n",
    "\n",
    "ğŸ‘ **Best for:**\n",
    "- Complex problems\n",
    "- Avoids overfitting better than single trees\n",
    "\n",
    "ğŸ‘ **Limitations:**\n",
    "- Less interpretable\n",
    "- Slower than single trees\n",
    "\n",
    "ğŸ” **Example Use:** Fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "forest-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)  # 100 trees\n",
    "forest.fit(X_train, y_train)\n",
    "accuracy = forest.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bayes-intro",
   "metadata": {},
   "source": [
    "## Model 5: Naive Bayes\n",
    "\n",
    "ğŸ“Š **How it works:** \n",
    "- Uses probability (Bayes' Theorem) ğŸ“ˆ\n",
    "- \"Naive\" because it assumes independence\n",
    "\n",
    "ğŸ‘ **Best for:**\n",
    "- Text classification\n",
    "- Very fast training\n",
    "\n",
    "ğŸ‘ **Limitations:**\n",
    "- Independence assumption often wrong\n",
    "\n",
    "ğŸ” **Example Use:** Sentiment analysis (positive/negative reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bayes-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "accuracy = nb.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "| Model            | Pros                      | Cons                      | Best For                  |\n",
    "|------------------|---------------------------|---------------------------|---------------------------|\n",
    "| Logistic Reg.    | Simple, fast              | Linear only               | Baseline models           |\n",
    "| k-NN             | Intuitive, no training    | Slow prediction           | Small, clear groupings    |\n",
    "| Decision Tree    | Easy to interpret         | Overfits easily           | Business rules            |\n",
    "| Random Forest    | Powerful, robust          | Complex, slower           | Complex problems          |\n",
    "| Naive Bayes      | Very fast, good for text  | Strong assumptions        | Text classification       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
